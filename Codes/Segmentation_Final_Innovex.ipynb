{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Segmentation_Final_Innovex.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jhn9EB-ZAIyk",
        "outputId": "bdb48500-c572-4dd2-aaff-ca0ef080732d"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np \n",
        "import os\n",
        "import glob\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "from skimage.util import img_as_ubyte\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "#from keras import optimizers\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.backend as K\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "from PIL import Image\n",
        "from keras.models import model_from_json\n",
        "#KERAS\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,Adam\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "# mlp for the circles problem with hinge loss\n",
        "from sklearn.datasets import make_circles\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, Masking, TimeDistributed\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.backend as K\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "from PIL import Image\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "#KERAS\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,Adam\n",
        "from skimage.util import img_as_ubyte\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "import glob\n",
        "import keras\n",
        "#from keras.layers.core import Layer\n",
        "#import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Input, concatenate, GlobalAveragePooling2D, AveragePooling2D, Flatten, UpSampling2D\n",
        "import numpy as np \n",
        "import math \n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import skimage.data\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "#DATA AUGMENTATION\n",
        "data_gen_args = dict(rotation_range=0.2,\n",
        "                    width_shift_range=0.05,\n",
        "                    height_shift_range=0.05,\n",
        "                    shear_range=0.05,\n",
        "                    zoom_range=0.05,\n",
        "                    horizontal_flip=True,\n",
        "                    fill_mode='nearest')\n",
        "\n",
        "\n",
        "\n",
        "def adjustData(img,mask,flag_multi_class,num_class):\n",
        "  if(flag_multi_class):\n",
        "    img = img / 255\n",
        "    mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
        "    new_mask = np.zeros(mask.shape + (num_class,))\n",
        "    for i in range(num_class):\n",
        "      #for one pixel in the image, find the class in mask and convert it into one-hot vector\n",
        "      #index = np.where(mask == i)\n",
        "      #index_mask = (index[0],index[1],index[2],np.zeros(len(index[0]),dtype = np.int64) + i) if (len(mask.shape) == 4) else (index[0],index[1],np.zeros(len(index[0]),dtype = np.int64) + i)\n",
        "      #new_mask[index_mask] = 1\n",
        "      new_mask[mask == i,i] = 1\n",
        "      new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
        "      mask = new_mask\n",
        "  elif(np.max(img) > 1):\n",
        "      img = img / 255\n",
        "      mask = mask / 255\n",
        "      mask[mask > 0.5] = 1\n",
        "      mask[mask <= 0.5] = 0\n",
        "  return (img,mask)\n",
        "\n",
        "def trainGenerator(batch_size,train_path,image_folder_train,mask_folder_train,\n",
        "                   data_gen_args,image_color_mode = \"grayscale\",\n",
        "                   mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",\n",
        "                   mask_save_prefix  = \"mask\",\n",
        "                   flag_multi_class = False,num_class = 2,save_to_dir = None,\n",
        "                   target_size = (256,256),seed = 1):\n",
        "  image_datagen_train = ImageDataGenerator(**data_gen_args)\n",
        "  mask_datagen_train = ImageDataGenerator(**data_gen_args)\n",
        "    \n",
        "  image_generator_train = image_datagen_train.flow_from_directory(\n",
        "train_path,\n",
        "classes = [image_folder_train],\n",
        "class_mode = None,\n",
        "color_mode = image_color_mode,\n",
        "target_size = target_size,\n",
        "batch_size = batch_size,\n",
        "save_to_dir = save_to_dir,\n",
        "save_prefix  = image_save_prefix,\n",
        "seed = seed)\n",
        "  \n",
        "  mask_generator_train = mask_datagen_train.flow_from_directory(train_path,classes = [mask_folder_train],\n",
        "        class_mode = None,\n",
        "        color_mode = mask_color_mode,\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = save_to_dir,\n",
        "        save_prefix  = mask_save_prefix,\n",
        "        seed = seed)\n",
        "  train_generator = zip(image_generator_train, mask_generator_train)\n",
        "  for (img,mask) in train_generator:\n",
        "    img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
        "    yield (img,mask)\n",
        "\n",
        "def valGenerator(batch_size,train_path,\n",
        "                 image_folder_val,mask_folder_val,\n",
        "                 data_gen_args,image_color_mode = \"grayscale\",\n",
        "                 mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",\n",
        "                 mask_save_prefix  = \"mask\",\n",
        "                 flag_multi_class = False,num_class = 2, save_to_dir = None,\n",
        "                 target_size = (256,256),seed = 1):\n",
        "  image_datagen_val = ImageDataGenerator(**data_gen_args)\n",
        "  mask_datagen_val = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "\n",
        "  image_generator_val = image_datagen_val.flow_from_directory(\n",
        "        train_path,\n",
        "        classes = [image_folder_val],\n",
        "        class_mode = None,\n",
        "        color_mode = image_color_mode,\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = save_to_dir,\n",
        "        save_prefix  = image_save_prefix,\n",
        "        seed = seed)\n",
        "  mask_generator_val = mask_datagen_val.flow_from_directory(\n",
        "        train_path,\n",
        "        classes = [mask_folder_val],\n",
        "        class_mode = None,\n",
        "        color_mode = mask_color_mode,\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = save_to_dir,\n",
        "        save_prefix  = mask_save_prefix,\n",
        "        seed = seed)\n",
        "  val_generator = zip(image_generator_val, mask_generator_val)\n",
        "  for (img,mask) in val_generator:\n",
        "    img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
        "    yield (img,mask)\n",
        "\n",
        "\n",
        "\n",
        "def testGenerator(test_path, num_image, target_size = (256,256), flag_multi_class = False,as_gray = True):\n",
        "    for i in range(num_image):\n",
        "      img = io.imread(os.path.join(test_path,\"%d.png\"%i),as_gray = as_gray)\n",
        "      img = img / 255\n",
        "      img = trans.resize(img,target_size)\n",
        "      img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
        "      img = np.reshape(img,(1,)+img.shape)\n",
        "      yield img\n",
        "\n",
        "\n",
        "def geneTrainNpy(image_path,mask_path,flag_multi_class = False,num_class = 2,image_prefix = \"image\",mask_prefix = \"mask\",image_as_gray = True,mask_as_gray = True):\n",
        "  image_name_arr = glob.glob(os.path.join(image_path,\"%s*.png\"%image_prefix))\n",
        "  image_arr = []\n",
        "  mask_arr = []\n",
        "  for index,item in enumerate(image_name_arr):\n",
        "    img = io.imread(item,as_gray = image_as_gray)\n",
        "    img = np.reshape(img,img.shape + (1,)) if image_as_gray else img\n",
        "    mask = io.imread(item.replace(image_path,mask_path).replace(image_prefix,mask_prefix),as_gray = mask_as_gray)\n",
        "    mask = np.reshape(mask,mask.shape + (1,)) if mask_as_gray else mask\n",
        "    img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
        "    image_arr.append(img)\n",
        "    mask_arr.append(mask)\n",
        "    image_arr = np.array(image_arr)\n",
        "    mask_arr = np.array(mask_arr)\n",
        "    return image_arr,mask_arr\n",
        "\n",
        "\n",
        "def labelVisualize(num_class,color_dict,img):\n",
        "  img = img[:,:,0] if len(img.shape) == 3 else img\n",
        "  img_out = np.zeros(img.shape + (3,))\n",
        "  for i in range(num_class):\n",
        "    img_out[img == i,:] = color_dict[i]\n",
        "    return img_out / 255\n",
        "def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
        "  for i,item in enumerate(npyfile):\n",
        "    img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
        "    cv2.imwrite(os.path.join(save_path,\"%d_predict.png\"%i),img_as_ubyte(img))\n",
        "    #io.imsave(os.path.join(save_path,\"%d_predict.png\"%i),img)\n",
        "\n",
        "def dice_coef(label, results):\n",
        "  y_true_f = K.flatten(label)\n",
        "  y_pred_f = K.flatten(results)\n",
        "  intersection = K.sum(y_true_f * y_pred_f)\n",
        "  return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n",
        "\n",
        "def jaccard_distance(label, results, smooth=100):\n",
        "  intersection = K.sum(K.abs(label * results), axis=-1)\n",
        "  sum_ = K.sum(K.abs(label) + K.abs(results), axis=-1)\n",
        "  jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "  return (1 - jac) * smooth        \n",
        "\n",
        "def down_block(x, filters, kernel_size=(3,3), padding = \"same\", strides=1):\n",
        "  c = tf.keras.layers.Conv2D(filters, kernel_size, padding = padding, strides = strides, activation = \"relu\")(x)\n",
        "  c = tf.keras.layers.BatchNormalization()(c)\n",
        "  c1c = tf.keras.layers.Conv2D(filters, kernel_size, padding = padding, strides = strides, activation = \"relu\")(c)\n",
        "  p = tf.keras.layers.MaxPool2D((2,2),(2,2))(c1c)\n",
        "    \n",
        "  return c, p\n",
        "\n",
        "def up_block(x, skip, filters, kernel_size=(3,3), padding = \"same\", strides=1):\n",
        "  us = tf.keras.layers.UpSampling2D((2,2))(x)\n",
        "  concat = tf.keras.layers.Concatenate()([us, skip])\n",
        "  c = tf.keras.layers.BatchNormalization()(concat)\n",
        "  c = tf.keras.layers.Conv2DTranspose(filters, kernel_size, padding = padding, strides = strides, activation = \"relu\")(concat)\n",
        "  c = tf.keras.layers.Conv2DTranspose(filters, kernel_size, padding = padding, strides = strides, activation = \"relu\")(c)\n",
        "  return c\n",
        "\n",
        "def bottelneck(x, filters, kernel_size=(3,3), padding = \"same\", strides=1):\n",
        "  c = tf.keras.layers.Conv2D(filters, kernel_size, padding = padding, strides = strides, activation = \"relu\")(x)\n",
        "  c = tf.keras.layers.Conv2D(filters, kernel_size, padding = padding, strides = strides, activation = \"relu\")(c)\n",
        "  return c\n",
        "\n",
        "####################################################End of Different Convolutional Blocks ###############################################\n",
        "    \n",
        "#################################################### U-NET Model ########################################################################    \n",
        "    \n",
        "def Unet():\n",
        "  f = [16, 32, 64, 128, 256] ####feature maps, No. of Filters\n",
        "  image_size= 256\n",
        "    #droprate = 0.5\n",
        "  inputs = tf.keras.layers.Input((image_size, image_size, 1)) #####Input to the Model\n",
        "    \n",
        "  p0 = inputs  \n",
        "    ##### Input is assigned to p0 \n",
        "    ######Downsampling the Images                                     \n",
        "  c1, p1 = down_block(p0, f[0])  #### 128 ---> 64\n",
        "  c1 = tf.keras.layers.Dropout(0.5)(c1)\n",
        "\n",
        "  c2, p2 = down_block(p1, f[1])  #### 64----> 32\n",
        "  c2 = tf.keras.layers.Dropout(0.5)(c2)\n",
        "\n",
        "  c3, p3 = down_block(p2, f[2])  #### 32---> 16\n",
        "  c3 = tf.keras.layers.Dropout(0.5)(c3)\n",
        "\n",
        "  c4, p4 = down_block(p3, f[3])  #### 16----> 8\n",
        "  c4 = tf.keras.layers.Dropout(0.5)(c4)\n",
        "\n",
        "  c5, p5 = down_block(p4, f[4])  #### 16----> 8\n",
        "  c5 = tf.keras.layers.Dropout(0.5)(c5)\n",
        "\n",
        "    \n",
        "  bn = bottelneck(p5, f[4])    \n",
        "    \n",
        "    ######Upsampling the Images                                     \n",
        "  u1 = up_block(bn, c5, f[4])\n",
        "  u1 = tf.keras.layers.Dropout(0.5)(u1)\n",
        "\n",
        "  u2 = up_block(u1, c4, f[3])\n",
        "  u2 = tf.keras.layers.Dropout(0.5)(u2)\n",
        "\n",
        "\n",
        "  u3 = up_block(u2, c3, f[2])\n",
        "  u3 = tf.keras.layers.Dropout(0.5)(u3)\n",
        "\n",
        "  u4 = up_block(u3, c2, f[1])\n",
        "  u4 = tf.keras.layers.Dropout(0.5)(u4)\n",
        "\n",
        "  u5 = up_block(u4, c1, f[0])  \n",
        "  u5 = tf.keras.layers.Dropout(0.5)(u5)\n",
        "    \n",
        "    #########output Layer\n",
        "  outputs = tf.keras.layers.Conv2D(1, (1,1), padding = \"same\", activation = \"sigmoid\")(u5)\n",
        "    \n",
        "    ########preparation of Model\n",
        "  model = tf.keras.models.Model(inputs, outputs)\n",
        "    \n",
        "#    from keras import optimizers\n",
        "    #import keras.backend as K\n",
        "  sgd = SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    #adam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "    \n",
        "    #model.compile(optimizer = adam1, loss = \"binary_crossentropy\", metrics = [\"acc\"])\n",
        "   \n",
        "  model.compile(optimizer= sgd,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "#################################################### End of U-NET Model #################################################################    \n",
        "\n",
        "train_path = \"/content/drive/MyDrive/Minita/\"\n",
        "image_folder_train = \"image_folder_train\" \n",
        "mask_folder_train =  \"mask_train\"\n",
        "image_folder_val = \"image_val\" \n",
        "mask_folder_val =  \"mask_val\"\n",
        "\n",
        "#augmentaed_images_train = \"D:/train_test_convert/Inception/Inception/Inception/New_Train/Auggmented_Images_train\"\n",
        "#augmentaed_images_val = \"D:/train_test_convert/Inception/Inception/Inception/New_Train/Auggmented_Images_val\"\n",
        "\n",
        "batch_size = 4\n",
        "steps_per_epoch = 76#TotalTrainingSamples / TrainingBatchSize\n",
        "epochs = 100\n",
        "#period = 3\n",
        "validation_steps = 29   #TotalvalidationSamples / ValidationBatchSize\n",
        "\n",
        "\n",
        "print(\"*****************Started Training******************\")\n",
        "\n",
        "myGene_train = trainGenerator(batch_size, train_path, image_folder_train, mask_folder_train,\n",
        "                              data_gen_args, save_to_dir = None)\n",
        "\n",
        "myGene_val   = valGenerator(batch_size, train_path, image_folder_val, mask_folder_val,\n",
        "                            data_gen_args, save_to_dir = None)\n",
        "\n",
        "model = Unet()\n",
        "\n",
        "H=model.fit(myGene_train,\n",
        "                      steps_per_epoch = steps_per_epoch,\n",
        "                      epochs = epochs, \n",
        "                      validation_data = myGene_val,\n",
        "                      validation_steps = validation_steps)\n",
        "print(\"Saving the Model.........................\")\n",
        "model.save('/content/drive/MyDrive/Minita/Model/minita.h5')\n",
        "print(\"Model Saved in Drive......................\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****************Started Training******************\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 256, 256, 16) 160         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 256, 256, 16) 64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 256, 256, 16) 2320        batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 128, 128, 16) 0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 32) 4640        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 128, 128, 32) 9248        batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 128)  147584      batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 256)  590080      batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 256)    590080      max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 256)    590080      conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 16, 16, 256)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 16, 16, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 16, 16, 512)  0           up_sampling2d[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 16, 16, 256)  1179904     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 16, 16, 256)  590080      conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 16, 16, 256)  0           conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 256)  0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 384)  0           up_sampling2d_1[0][0]            \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 32, 32, 128)  442496      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 128)  147584      conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 128)  0           conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 128)  0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 64, 64, 192)  0           up_sampling2d_2[0][0]            \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 64, 64, 64)   110656      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 64)   36928       conv2d_transpose_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 64, 64, 64)   0           conv2d_transpose_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 64) 0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 128, 128, 96) 0           up_sampling2d_3[0][0]            \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTrans (None, 128, 128, 32) 27680       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 128, 128, 32) 9248        conv2d_transpose_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 128, 128, 32) 0           conv2d_transpose_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 256, 256, 32) 0           dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 256, 256, 16) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 256, 256, 48) 0           up_sampling2d_4[0][0]            \n",
            "                                                                 dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 256, 256, 16) 6928        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTrans (None, 256, 256, 16) 2320        conv2d_transpose_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 256, 256, 16) 0           conv2d_transpose_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 256, 256, 1)  17          dropout_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,914,465\n",
            "Trainable params: 4,913,473\n",
            "Non-trainable params: 992\n",
            "__________________________________________________________________________________________________\n",
            "Found 0 images belonging to 1 classes.\n",
            "Found 0 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4269a4541bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    375\u001b[0m                       \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                       \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyGene_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                       validation_steps = validation_steps)\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving the Model.........................\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Minita/Model/minita.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    834\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-4269a4541bea>\u001b[0m in \u001b[0;36mtrainGenerator\u001b[0;34m(batch_size, train_path, image_folder_train, mask_folder_train, data_gen_args, image_color_mode, mask_color_mode, image_save_prefix, mask_save_prefix, flag_multi_class, num_class, save_to_dir, target_size, seed)\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_generator_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_generator_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjustData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflag_multi_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-4269a4541bea>\u001b[0m in \u001b[0;36madjustData\u001b[0;34m(img, mask, flag_multi_class, num_class)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0mnew_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mflag_multi_class\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m   \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2704\u001b[0m     \"\"\"\n\u001b[1;32m   2705\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2706\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t7cCLW_D01l",
        "outputId": "ba8930a0-e223-4312-f1c6-400fe0eb4ba2"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np \n",
        "import os\n",
        "import glob\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "from skimage.util import img_as_ubyte\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "#from keras import optimizers\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.backend as K\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "from PIL import Image\n",
        "from keras.models import model_from_json\n",
        "#KERAS\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,Adam\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "# mlp for the circles problem with hinge loss\n",
        "from sklearn.datasets import make_circles\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, Masking, TimeDistributed\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.backend as K\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "from PIL import Image\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "#KERAS\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,Adam\n",
        "from skimage.util import img_as_ubyte\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "import glob\n",
        "import keras\n",
        "#from keras.layers.core import Layer\n",
        "#import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Input, concatenate, GlobalAveragePooling2D, AveragePooling2D, Flatten, UpSampling2D\n",
        "import numpy as np \n",
        "import math \n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import skimage.data\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "import cv2\n",
        "\n",
        "\n",
        "def testGenerator(test_path, num_image, target_size = (256,256), flag_multi_class = False,as_gray = True):\n",
        "    for i in range(num_image):\n",
        "      img = io.imread(os.path.join(test_path,\"%d.png\"%i),as_gray = as_gray)\n",
        "      img = img / 255\n",
        "      img = trans.resize(img,target_size)\n",
        "      img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
        "      img = np.reshape(img,(1,)+img.shape)\n",
        "      yield img\n",
        "\n",
        "\n",
        "\n",
        "def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
        "  for i,item in enumerate(npyfile):\n",
        "    img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
        "    cv2.imwrite(os.path.join(save_path,\"%d_predict.png\"%i),img_as_ubyte(img))\n",
        "    #io.imsave(os.path.join(save_path,\"%d_predict.png\"%i),img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Loading saved Model\")\n",
        "loaded_model = tf.keras.models.load_model(\"/content/drive/MyDrive/Minita/Model/Model_Mammo_400_epochs_SGD_Optimizer.h5\")\n",
        "test_images = 28\n",
        "test_path = \"/content/drive/MyDrive/Minita/test_images\"\n",
        "Predicted_path = \"/content/drive/MyDrive/Minita/preds_minita\"\n",
        "test_label = \"/content/drive/MyDrive/Minita/minita\"\n",
        "print(\"testing images on model.............\")\n",
        "H1=testGenerator(test_path,test_images)\n",
        "results = loaded_model.predict_generator(H1,test_images,verbose=1)\n",
        "print(\"Predicted images saved.......\")\n",
        "saveResult(Predicted_path,results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading saved Model\n",
            "testing images on model.............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2001: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 29s 1s/step\n",
            "Predicted images saved.......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "_ihlq7RPw8TG",
        "outputId": "b74bffe8-1828-4b86-8849-83904aec7dda"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "og = cv2.imread('/content/drive/MyDrive/Minita/test_images/22.png')\n",
        "og=cv2.resize(og, (256,256))\n",
        "img = cv2.imread('/content/drive/MyDrive/Minita/preds_minita/22_predict.png',0)\n",
        "ret,thresh1 = cv2.threshold(img,16,255,cv2.THRESH_BINARY)\n",
        "gt = cv2.imread('/content/drive/MyDrive/Minita/Ground_Truth/Ground_Truth (22).png',0)\n",
        "\n",
        "img2 = cv2.resize(gt, (256,256))\n",
        "\n",
        "titles = ['Original','Trained','Ground Truth','Predicted']\n",
        "images = [og,img, img2, thresh1]\n",
        "\n",
        "\n",
        "for i in range(4):\n",
        "    plt.subplot(1,4,i+1),plt.imshow(images[i],'gray')\n",
        "    plt.title(titles[i])\n",
        "    plt.xticks([]),plt.yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP0aGxD2oFx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0096b8bc-3769-4107-c4cd-f0f4f8300b3b"
      },
      "source": [
        "#Calculating DSC & IOU/Jaccard Coefficients\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.metrics import jaccard_similarity_score\n",
        "import keras.backend as K\n",
        "\n",
        "true = cv2.imread('/content/drive/MyDrive/dice_coef/1.jpeg')\n",
        "pred = cv2.imread('/content/drive/MyDrive/dice_coef/2.jpeg')\n",
        "print('Original Dimensions : ',pred.shape)\n",
        "\n",
        "width = 256\n",
        "height = 256\n",
        "dim = (width, height)\n",
        "  \n",
        "# resize image\n",
        "true = cv2.resize(true, dim, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "print('Resized Dimensions : ',true.shape)\n",
        "\n",
        "pred = cv2.resize(pred, dim, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "print('Resized Dimensions : ',pred.shape)\n",
        "\n",
        "def single_dice_coef(y_true, y_pred_bin):\n",
        "    # shape of y_true and y_pred_bin: (height, width)\n",
        "    intersection = np.sum(y_true * y_pred_bin)\n",
        "    if (np.sum(y_true)==0) and (np.sum(y_pred_bin)==0):\n",
        "        return 1\n",
        "    return (2*intersection) / (np.sum(y_true) + np.sum(y_pred_bin))\n",
        "\n",
        "def mean_dice_coef(y_true, y_pred_bin):\n",
        "    # shape of y_true and y_pred_bin: (n_samples, height, width, n_channels)\n",
        "    batch_size = y_true.shape[0]\n",
        "    channel_num = y_true.shape[-1]\n",
        "    mean_dice_channel = 0.\n",
        "    for i in range(batch_size):\n",
        "        for j in range(channel_num):\n",
        "            channel_dice = single_dice_coef(y_true[i, :, j], y_pred_bin[i, :, j])\n",
        "            mean_dice_channel += channel_dice/(channel_num*batch_size)\n",
        "    return mean_dice_channel\n",
        "\n",
        "print(\"The Dice Coefficient is: \",mean_dice_coef(true, pred))\n",
        "\n",
        "true=np.array(true).ravel()\n",
        "pred=np.array(pred).ravel()\n",
        "iou = jaccard_similarity_score(true, pred)\n",
        "\n",
        "print(\"The IOU/Jaccard Coefficient is: \",iou)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Dimensions :  (256, 256, 3)\n",
            "Resized Dimensions :  (256, 256, 3)\n",
            "Resized Dimensions :  (256, 256, 3)\n",
            "The Dice Coefficient is:  0.8272399969403018\n",
            "The IOU/Jaccard Coefficient is:  0.98992919921875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:664: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ2qviGsRaK5"
      },
      "source": [
        "'''import glob\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "count=0\n",
        "for img in glob.glob(\"/content/drive/MyDrive/Minita/preds_minita/*.png\"):\n",
        "  \n",
        "  filename = img\n",
        "  path = \"/content/drive/MyDrive/Minita/binary_minita\" \n",
        "  img = cv2.imread(img)\n",
        "  ret, bw_img = cv2.threshold(img,15,255,cv2.THRESH_BINARY)\n",
        "  cv2.imwrite(os.path.join(path , 'binary%d.png'%count), bw_img)\n",
        "  count=count+1'''\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeX9Sn5afizF",
        "outputId": "9283548c-8e19-47ec-a128-6c36f396c246"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}